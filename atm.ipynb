{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXivTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "from IPython.display import HTML\n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class topic_modelling():\n",
    "    def __init__(self):\n",
    "        self.params = {\n",
    "            'data_source': None,\n",
    "            'json_prefix': '',\n",
    "            'scope': ['title', 'abstract'],\n",
    "            'language': 'english',\n",
    "            'remove_stopwords': True,\n",
    "            'additional_stopwords': set(),\n",
    "            'stem_words': False,\n",
    "            'lemmatize_words': True,\n",
    "            'tfidf': True,\n",
    "            'bigrams': True,\n",
    "            'bigram_min': 2,\n",
    "            'bigram_threshold': 3,\n",
    "            'trigrams': True,\n",
    "            'trigram_min': 2,\n",
    "            'trigram_threshold': 3\n",
    "        }\n",
    "        self.data = {}\n",
    "        \n",
    "    def set_project_parameters(self, **kwargs):              \n",
    "        for k, v in kwargs.items():\n",
    "            if k == 'params':\n",
    "                assert type(v) is dict, \"params need to be passed as dict\"\n",
    "                self.params.update(params)\n",
    "            else:\n",
    "                self.params[k] = v\n",
    "    \n",
    "    def read_json(self):\n",
    "        if self.params['data_source'] is None:\n",
    "            print('Error. You need to set a data source in the params first')\n",
    "            return\n",
    "        if self.params['scope'] is None:\n",
    "            print('Error. You need to set the scope of the data in the params first.')\n",
    "        \n",
    "        with open(self.params['data_source']) as infile:\n",
    "            json_data = json.load(infile)\n",
    "        data = []\n",
    "        for k, i in json_data.items():\n",
    "            identifier = k\n",
    "            for item_type, item_value in i.items():\n",
    "                data_value = ''\n",
    "                prefix = self.params['json_prefix']\n",
    "                if item_type[len(prefix):] in self.params['scope']:\n",
    "                    data_value += item_value + ' '\n",
    "            data.append([identifier, data_value])\n",
    "        self.data['raw'] = data\n",
    " \n",
    "    def preprocess_text(self, data):\n",
    "        # Remove line breaks\n",
    "        data.replace('\\n',' ')\n",
    "        data = re.sub(' +',' ', data)\n",
    "        \n",
    "        # Tokenize data\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        data = tokenizer.tokenize(data.lower())\n",
    "        \n",
    "        # Remove numbers, but not words that contain numbers.\n",
    "        #data = [token for token in data if not token.isdigit()]\n",
    "\n",
    "        #if self.params['remove_stopwords']:\n",
    "        #    stop = set(stopwords.words(self.params['language']))\n",
    "        #    for i in self.params['additional_stopwords']:\n",
    "        #        stop.add(i)\n",
    "        #    data = [token for token in data if not token in stop]\n",
    "\n",
    "        if self.params['stem_words']:\n",
    "            stemmer = SnowballStemmer(self.param['language'])\n",
    "            data = [stemmer.stem(token) for token in data]\n",
    "\n",
    "        if self.params['lemmatize_words']:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            data = [lemmatizer.lemmatize(token) for token in data]\n",
    "            \n",
    "        if self.params['bigrams']:\n",
    "            bigram = self.params['bigram_phraser']\n",
    "            data = bigram[data]\n",
    "        \n",
    "        if self.params['bigrams'] and self.params['trigrams']:\n",
    "            trigram = self.params['trigram_phraser'] \n",
    "            data = trigram[data]\n",
    "        elif trigrams and not bigrams:\n",
    "            print('Error. Trigrams requires Bigrams.')\n",
    "        return data\n",
    "        \n",
    "    def preprocess_corpus(self): \n",
    "        # Remove line breaks and double spaces\n",
    "        data = self.data['raw']\n",
    "        \n",
    "        data = [[item[0], item[1].replace('\\n',' ')] for item in data]\n",
    "        data = [[item[0], re.sub(' +',' ', item[1])] for item in data]\n",
    "\n",
    "        # Tokenize data\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        data = [[item[0], tokenizer.tokenize(item[1].lower())] for item in data]\n",
    "\n",
    "        # Remove numbers, but not words that contain numbers.\n",
    "        data = [[item[0], [token for token in item[1] if not token.isdigit()]] for item in data]\n",
    "\n",
    "        if self.params['remove_stopwords']:\n",
    "            stop = set(stopwords.words(self.params['language']))\n",
    "            for i in self.params['additional_stopwords']:\n",
    "                stop.add(i)\n",
    "            data = [[item[0], [token for token in item[1] if not token in stop]] for item in data]\n",
    "\n",
    "        if self.params['stem_words']:\n",
    "            stemmer = SnowballStemmer(self.params['language'])\n",
    "            data = [[item[0], [stemmer.stem(token) for token in item[1]]] for item in data]\n",
    "\n",
    "        if self.params['lemmatize_words']:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            data = [[item[0], [lemmatizer.lemmatize(token) for token in item[1]]] for item in data]\n",
    "\n",
    "        data = [list(item) for item in list(list(zip(*data)))]\n",
    "        \n",
    "        if self.params['bigrams']:\n",
    "            bi_phrases = Phrases(data[1], min_count=self.params['bigram_min'], \n",
    "                                 threshold=self.params['bigram_threshold'])\n",
    "            bigram = Phraser(bi_phrases)\n",
    "            data = [data[0], [bigram[d] for d in data[1]]]\n",
    "            self.params['bigram_phraser'] = bigram\n",
    "        if self.params['bigrams'] and self.params['trigrams']:\n",
    "            tri_phrases = Phrases(bigram[data[1]], min_count=self.params['trigram_min'], \n",
    "                                  threshold=self.params['trigram_threshold'])\n",
    "            trigram = Phraser(tri_phrases)\n",
    "            self.params['trigram_phraser'] = trigram\n",
    "            data = [data[0], [trigram[d] for d in data[1]]]\n",
    "        elif trigrams and not bigrams:\n",
    "            print('Error. Trigrams requires Bigrams.')\n",
    "        self.data['corpus'] = data\n",
    "          \n",
    "    def lda(self, num_topics=10, passes=50):    \n",
    "        texts = self.data['corpus'][1]\n",
    "        # create dictionary of terms\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        print('dictionary created')\n",
    "\n",
    "        # convert tokenized documents into a document-term matrix\n",
    "        dtm = [dictionary.doc2bow(text) for text in texts]\n",
    "        print('doctument-term matrix created')\n",
    "\n",
    "        if self.params['tfidf']:\n",
    "            tfidf = models.TfidfModel(dtm)\n",
    "            dtm = tfidf[dtm]\n",
    "            print('tf-idf created')\n",
    "\n",
    "        # generate LDA model\n",
    "        ldamodel = models.ldamodel.LdaModel(dtm, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "        print('lda-model created')\n",
    "\n",
    "        if 'lda' not in self.data.keys():\n",
    "            self.data['lda'] = {}\n",
    "        \n",
    "        self.data['lda'][num_topics] = {\n",
    "            'model': ldamodel,\n",
    "            'dtm': dtm,\n",
    "            'dictionary': dictionary\n",
    "        }\n",
    "        return ldamodel, dtm, dictionary\n",
    "    \n",
    "    def ldavis(self, num_topics):\n",
    "        if num_topics in self.data['lda'].keys():\n",
    "            ldamodel = self.data['lda'][num_topics]['model']\n",
    "            dtm = self.data['lda'][num_topics]['dtm']\n",
    "            dictionary = self.data['lda'][num_topics]['dictionary']\n",
    "        \n",
    "            vis_data = pyLDAvis.gensim.prepare(ldamodel, dtm, dictionary)\n",
    "            vis = pyLDAvis.display(vis_data)\n",
    "            return vis\n",
    "        else:\n",
    "            print('No model for this number of topics has been generated.')\n",
    "    \n",
    "    def get_topics_by_term(self, num_topics=None, num_words=10):\n",
    "        if num_topics is None:\n",
    "            print('Error. You need to pass the number of topics.')\n",
    "            return\n",
    "        elif num_topics not in self.data['lda'].keys():\n",
    "            print('No model for this number of topics has been generated.')\n",
    "            return\n",
    "        else:\n",
    "            ldamodel = self.data['lda'][num_topics]['model']\n",
    "            dictionary = self.data['lda'][num_topics]['dictionary']\n",
    "        \n",
    "        \n",
    "        topical_terms = set()\n",
    "        for topic in ldamodel.show_topics(num_words=num_words, formatted=False):\n",
    "            for word in topic[1]:\n",
    "                topical_terms.add(word[0])\n",
    "        \n",
    "        tbt = {}\n",
    "        for topic in ldamodel.show_topics(num_words=10000, formatted=False):\n",
    "            tbt[topic[0]] = {}\n",
    "            for word in topical_terms:\n",
    "                tbt[topic[0]][word] = 0\n",
    "            for word in topic[1]:\n",
    "                if word[0] in topical_terms:\n",
    "                        tbt[topic[0]][word[0]] = word[1]\n",
    "            \n",
    "        self.data['lda'][num_topics]['tbt'] = pd.DataFrame.from_dict(tbt)\n",
    "        #print(tbt)\n",
    "        return pd.DataFrame.from_dict(tbt)\n",
    "        \n",
    "        '''\n",
    "        topics_by_doc = {}\n",
    "        for i in range(len(doc_data[0])):\n",
    "            tmp_bow = dictionary.doc2bow(doc_data[1][i])\n",
    "            topics_by_doc[doc_data[0][i]] = {\n",
    "            'id': doc_data[0][i],\n",
    "            'abstract': doc_data[1][i],\n",
    "            'topics': ldamodel.get_document_topics(tmp_bow)\n",
    "            }\n",
    "        '''\n",
    "        \n",
    "    def tbt_plot(self, style='line', num_topics=None, num_words=10, aspect=3):\n",
    "        if not num_topics in self.data['lda'].keys():\n",
    "            print('Error. Not yet...')\n",
    "            return\n",
    "        if not 'tbt' in self.data['lda'][num_topics]:\n",
    "            self.get_topics_by_term(num_topics=num_topics, num_words=num_words)\n",
    "        if style == 'line':\n",
    "            viz = sns.relplot(data=self.data['lda'][num_topics]['tbt'], aspect=aspect, \n",
    "                              kind='line', dashes=False, markers=True, sort=False) \n",
    "            viz.set_xticklabels(rotation=60, ha='right')\n",
    "        elif style == 'vertical':\n",
    "            terms = list(self.data['lda'][num_topics]['tbt'][0].keys())\n",
    "            topics = list(self.data['lda'][num_topics]['tbt'].keys())\n",
    "            topics_str = [str(x) for x in list(self.data['lda'][num_topics]['tbt'].keys())]\n",
    "            values = self.data['lda'][num_topics]['tbt']\n",
    "            \n",
    "            fig, ax = plt.subplots(1, len(topics), figsize=(18,10), sharey=True)\n",
    "            palette = plt.get_cmap('Set1')\n",
    "            num=0\n",
    "            for i in range(len(topics)):\n",
    "                num+=1\n",
    "                ax[i].plot(values[i], terms, marker='', color=palette(num), linewidth=1, alpha=0.7) #, label=column)\n",
    "                ax[i].fill_betweenx(terms, values[i], color=palette(num), alpha=0.2)\n",
    "    \n",
    "    def topics_by_document(self, num_topics=None):\n",
    "        if num_topics not in self.data['lda'].keys():\n",
    "            print('Error. Pass...')\n",
    "            return\n",
    "        ldamodel = self.data['lda'][num_topics]['model']\n",
    "        dictionary = self.data['lda'][num_topics]['dictionary']\n",
    "        results = {}\n",
    "        for i in range(num_topics):\n",
    "            results[i] = {}\n",
    "            for doc in self.data['raw']:\n",
    "                results[i][doc[0]] = 0\n",
    "        for doc in self.data['raw']:\n",
    "            text = self.preprocess_text(doc[1])    \n",
    "            tmp_bow = dictionary.doc2bow(text)\n",
    "            tmp_results = ldamodel.get_document_topics(tmp_bow)\n",
    "            for t in tmp_results:\n",
    "                results[t[0]][doc[0]] = t[1]\n",
    "        self.data['lda'][num_topics]['tbd'] = pd.DataFrame.from_dict(results)\n",
    "        return self.data['lda'][num_topics]['tbd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize topic modelling project and set variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'\n",
    "data_file = 'arXiv_coprus_topics_transparency.json'\n",
    "\n",
    "params = {\n",
    "    'data_source': os.path.join(base_dir, data_file),\n",
    "    'data_raw': None,\n",
    "    'json_prefix': 'http://arxiv.org/OAI/arXiv/:',\n",
    "    'scope': ['title', 'abstract'],\n",
    "    'language': 'english',\n",
    "    'remove_stopwords': True,\n",
    "    'additional_stopwords': set(),\n",
    "    'stem_words': False,\n",
    "    'lemmatize_words': True,\n",
    "    'tfidf': False,\n",
    "    'bigrams': True,\n",
    "    'bigram_min': 2,\n",
    "    'bigram_threshold': 3,\n",
    "    'trigrams': True,\n",
    "    'trigram_min': 2,\n",
    "    'trigram_threshold': 3\n",
    "}\n",
    "\n",
    "atm = topic_modelling()\n",
    "atm.set_project_parameters(params=params)\n",
    "atm.read_json()\n",
    "atm.preprocess_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel, dtm, dictionary = atm.lda(num_topics=4, passes=200) #, passes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm.ldavis(num_topics=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm.get_topics_by_term(num_topics=4, num_words=10)\n",
    "atm.tbt_plot(num_topics=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atm.tbt_plot(num_topics=4, style='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd = atm.topics_by_document(num_topics=4)\n",
    "vis = sns.relplot(data=tbd, aspect=3.5)\n",
    "vis.set_xticklabels(rotation=60, ha='right')\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tbd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tbd[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tbd[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tbd[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(tbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = sns.relplot(data=tbd.sort_values([2], ascending=False), aspect=3.5)\n",
    "vis.set_xticklabels(rotation=60, ha='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirty implementation of In Text Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 4\n",
    "largest = 10\n",
    "num_words = 100\n",
    "topic = 0\n",
    "\n",
    "top_tbd = {}\n",
    "for i in tbd.keys():\n",
    "    top_tbd[i] = list(tbd.nlargest(largest, i)[0].keys())\n",
    "\n",
    "atm.get_topics_by_term(num_topics=num_topics, num_words=num_words)\n",
    "tbt = atm.data['lda'][num_topics]['tbt']\n",
    "\n",
    "top_tbt = {}\n",
    "for i in tbt.keys():\n",
    "    top_tbt[i] = list(tbt.nlargest(num_words, i)[0].keys())\n",
    "\n",
    "def construct_text_viz(top_tbd, topic, mapper):\n",
    "    results = '<table>'\n",
    "    for doc_id in top_tbd[topic]:\n",
    "        for item in atm.data['raw']:\n",
    "            if item[0] == doc_id:\n",
    "                rtext = item[1]\n",
    "                ptext = atm.preprocess_text(rtext)\n",
    "                stext = []\n",
    "                for word in ptext:\n",
    "                    if word in top_tbt[0]:\n",
    "                        w = '<span style=\\\"background-color:'+str(matplotlib.colors.to_hex(mapper.to_rgba(tbt[0][word])))+';\\\"><b>'+word+'</b></span>'\n",
    "                        stext.append(w)\n",
    "                    else:\n",
    "                        stext.append(word)\n",
    "\n",
    "                stext = ' '.join(stext)\n",
    "                results += '<tr><td>'+str(doc_id)+'</td><td>'+rtext+'</td><td>'+stext+'</td></tr>'\n",
    "    results = results + '</table>'\n",
    "    return results\n",
    "\n",
    "\n",
    "results = '''<html><head>\n",
    "<style>\n",
    "html {font-family:Open Sans;}\n",
    "h1   {color: black;}\n",
    "table {border-spacing: 1.5rem;\n",
    "border: 1px solid black;}\n",
    "table {align:left;}\n",
    "td    {text-aligh: left;\n",
    "padding: 0.5rem;\n",
    "vertical-align: top;}\n",
    ".centered {\n",
    "  width: 70%;\n",
    "  margin: 0 auto;}\n",
    "</style></head><body>'''\n",
    "for topic in top_tbd.keys():\n",
    "    lst = list(tbt.nlargest(num_words, i)[topic])\n",
    "    minima = min(lst)\n",
    "    maxima = max(lst)\n",
    "    norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
    "    mapper = cm.ScalarMappable(norm=norm, cmap=cm.autumn_r) #Hot)\n",
    "    tmp = construct_text_viz(top_tbd, topic, mapper)\n",
    "    results += '<div class=\"centered\"><h1>Topic '+str(topic)+'</h1>'+tmp+'</div>'\n",
    "results += '</body></html>'\n",
    "\n",
    "with open('textviztext.html', 'w') as outfile:\n",
    "    outfile.write(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potenzielle Verbesserungen\n",
    "\n",
    "## Pre Analysis\n",
    "\n",
    "## Filtern selterner und häufiger Worte\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))\n",
    "\n",
    "See: https://www.kaggle.com/ykhorramz/lda-and-t-sne-interactive-visualization\n",
    "\n",
    "# Weitere Analyse\n",
    "\n",
    "## Topics in Text\n",
    "\n",
    "See: https://www.kaggle.com/ykhorramz/lda-and-t-sne-interactive-visualization\n",
    "\n",
    "\n",
    "# Check out\n",
    "\n",
    "spacy library for language processing\n",
    "\n",
    "\n",
    "TOM Library for topic modelling and browsing (??? Whats the browsing part???)\n",
    "http://mediamining.univ-lyon2.fr/people/guille/tom.php\n",
    "https://github.com/AdrienGuille/TOM\n",
    "\n",
    "\n",
    "3d.js: \n",
    "Focus node explorer with d3.js\n",
    "http://ramblings.mcpher.com/Home/excelquirks/gassites/d3nodefocus\n",
    "\n",
    "## Network Visualization\n",
    "\n",
    "NetworkX\n",
    "https://www.udacity.com/wiki/creating-network-graphs-with-python\n",
    "\n",
    "\n",
    "### Create Netword data out of model:\n",
    "http://intelligentonlinetools.com/blog/2017/01/22/data-visualization-visualizing-an-lda-model-using-python/\n",
    "\n",
    "- The work with models created with Mallet\n",
    "\n",
    "Semantic Network\n",
    "In LDA, topics are clusters of terms that co-occur in documents. We can interpret an LDA topic model as a network of terms linked by their participation in particular topics. In Tethne, we call this a topic-coupling network.\n",
    "\n",
    "Requires tethne. But pip3 install fails!\n",
    "\n",
    "\n",
    "\n",
    "## Plotting words and documents in 2d\n",
    "Plotting words and documents in 2D with SVD\n",
    "We can use SVD with 2 components (topics) to display words and documents in 2D. The process is really similar. Let’s start with displaying documents since it’s a bit more straightforward.\n",
    "see: https://nlpforhackers.io/topic-modeling/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Tests \n",
    "\n",
    "**No: stemming, lemmatizing, bigrams, trigrams**\n",
    "\n",
    "```\n",
    "d = preprocess_corpus(data, lemmatize_words=False, stem_words=False, bigrams=False, trigrams=False)\n",
    "ldamodel, corpus, dictionary = lda(d[1])\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "```\n",
    "\n",
    "```\n",
    "d = preprocess_corpus(data, lemmatize_words=False, stem_words=False, bigrams=False, trigrams=False)\n",
    "ldamodel, corpus, dictionary = lda(d[1], num_topics=6)\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "```\n",
    "\n",
    "**Stemming: Yes, No: Lemmata, bigrams or trigrams**\n",
    "\n",
    "```\n",
    "d = preprocess_corpus(data, lemmatize_words=False, stem_words=True, bigrams=False, trigrams=False)\n",
    "ldamodel, corpus, dictionary = lda(d[1], num_topics=10)\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "\n",
    "d = preprocess_corpus(data, lemmatize_words=False, stem_words=True, bigrams=False, trigrams=False)\n",
    "ldamodel, corpus, dictionary = lda(d[1], num_topics=4)\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "```\n",
    "\n",
    "**Lemmatize, NO: Stemming, bigrams, trigrams**\n",
    "```\n",
    "d = preprocess_corpus(data, lemmatize_words=True, stem_words=False, bigrams=False, trigrams=False)\n",
    "ldamodel, corpus, dictionary = lda(d[1], num_topics=10)\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "\n",
    "d = preprocess_corpus(data, lemmatize_words=True, stem_words=False, bigrams=False, trigrams=False)\n",
    "ldamodel, corpus, dictionary = lda(d[1], num_topics=5)\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "```\n",
    "\n",
    "**Lemmatize, bigrams, trigrams. NO: Stemming**\n",
    "```\n",
    "d = preprocess_corpus(data, lemmatize_words=True, stem_words=False, bigrams=True, trigrams=True)\n",
    "ldamodel, corpus, dictionary = lda(d[1], num_topics=10)\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "\n",
    "d = preprocess_corpus(data, lemmatize_words=True, stem_words=False, bigrams=True, trigrams=True)\n",
    "ldamodel, corpus, dictionary = lda(d[1], num_topics=4)\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "\n",
    "d = preprocess_corpus(data, lemmatize_words=True, stem_words=False, bigrams=True, trigrams=True)\n",
    "ldamodel, corpus, dictionary = lda(d[1], num_topics=4, tfidf=True)\n",
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
